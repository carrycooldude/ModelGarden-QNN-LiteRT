# Gemma3n-QNN-LiteRT Android Chat

A premium on-device LLM chat application for Android, powered by **Google LiteRT (formerly TensorFlow Lite)** and running the **Gemma 3n E2B (Instructed)** model.

**Gemma 3n** is Google's latest family of models enabling efficient AI on everyday devices.
*   **"E2B"**: Effective 2 Billion parameters.
*   **Optimized**: Built with MatFormer architecture and Per-Layer Embedding (PLE) for high-speed edge inference.
*   **Performance**: Capable of 30-50+ tokens/sec on modern mobile processors.


## üöÄ Features

*   **Next-Gen Edge AI**: Runs the highly efficient **Gemma 3n E2B** model offline.
*   **LiteRT-LM Engine**: Latest Google AI Edge runtime for optimized generation.
*   **Hardware Acceleration**:
    *   **GPU**: Default OpenCL backend for broad compatibility and speed.
    *   **NPU (QNN)**: Experimental support included (requires compatible HTP backend binary).
*   **Modern Premium UI**:
    *   Deep Blue & Soft Gray aesthetic.
    *   Floating "pill" input bar with keyboard auto-adjustment.
    *   Native Android `RecyclerView` with smooth streaming updates.
    *   Custom vector avatars and message bubbles.
*   **Streaming Responses**: Real-time text generation with typing indicators.

## üõ†Ô∏è Setup & Installation

### Prerequisites
*   Android Studio Ladybug (or newer).
*   Android Device with GPU support (Android 10+ recommended).
*   ~2GB free storage for the model.

### 1. Clone the Repository
```bash
git clone https://github.com/carrycooldude/Gemma3n-QNN-LiteRT.git
cd Gemma3n-QNN-LiteRT
```

### 2. Download the Model
Due to size and licensing, the model is not included in the repo.
1.  Download the **`gemma-3n-E2B-it-litert-lm.bin`** (or compatible LiteRT model) from:
    *   [**HuggingFace: google/gemma-3n-E2B-it-litert-lm**](https://huggingface.co/google/gemma-3n-E2B-it-litert-lm)
2.  Rename the downloaded file to `model.bin`.

### 3. Build & Install
Open the project in Android Studio and build `assembleDebug`.
```bash
./gradlew assembleDebug
```

### 4. Push Model to Device
The app looks for the model at `/data/local/tmp/llm/model.bin`.
```bash
adb shell mkdir -p /data/local/tmp/llm/
adb push path/to/your/gemma-2b-it-gpu-int4.bin /data/local/tmp/llm/model.bin
```

## üì± Usage
1.  Launch the app.
2.  Wait for initialization (GPU delegates loading).
3.  Start chatting! The keyboard will automatically resize the chat window.

## ‚ö†Ô∏è Notes on QNN (NPU)
This project contains code for **Qualcomm QNN (NPU)** acceleration via LiteRT delegates.
*   Currently, the `gemma-3n` model binaries compatible with the specific QNN HTP backend version are limited.
*   The app defaults to **GPU** for maximum stability.
*   To experiment with NPU, modify `LiteRTLMManager.kt` to prioritize `Backend.NPU`.

## üìú License
Apache 2.0
